{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202f5a0-5070-4957-908f-807a226570e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e9f63-f114-4c33-b937-144793389b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] protobuf==3.20.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c5f6-2487-468f-8797-24b05424a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mss pydirectinput pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cb096-b11f-44ff-a7a3-ef858d90b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506770d0-7a19-4bea-9dbb-17c0866710d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline is for training the DQN to train the AI for the game\n",
    "#MSS is used for screen cap.\n",
    "#Pydirectinput is used for sending the commands.\n",
    "#OpenCV allows processing of the frames.\n",
    "#Numpy is used for carrying mathematical operations.\n",
    "#Tesseract is for OCR to identify when Game Over is displayed for each round.\n",
    "#Matplotlib is used for visualizing the frames captured during the training process.\n",
    "#Time is used for pausing the frames.\n",
    "#Gym is OpenAI framework for training AI for different games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55bfe6-ed62-4b84-9d3d-53ad55cd32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33fb9b-944c-4798-ac39-29c9f717a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the game environment from the scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc046462-952d-4371-b3ec-86cc50557d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoGame(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Creating the gaming space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,100), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)  #Defines the only three ations allowed for the Dino i.e. up, down and no action\n",
    "\n",
    "        #Capturing the extraction parameters of the game\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top': 300, 'left': 0, 'width': 600, 'height': 500}\n",
    "        self.done_location = {'top': 405, 'left': 630, 'width': 660, 'height': 70}\n",
    "\n",
    "    #Defining how the actions will be played by the agent\n",
    "    def step(self,action):\n",
    "        action_map = {\n",
    "            0:'space',\n",
    "            1: 'down', \n",
    "            2: 'no_op'\n",
    "        }\n",
    "        if action !=2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "        #Calculating the reward after the game is done\n",
    "        done, done_cap = self.get_done() \n",
    "        observation = self.get_observation()\n",
    "        reward = 1 \n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pydirectinput.click(x=150, y=150)\n",
    "        pydirectinput.press('space')\n",
    "        return self.get_observation()\n",
    "\n",
    "    #Visualizing the game\n",
    "    def render(self):\n",
    "        cv2.imshow('Game', self.current_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "         \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    #Preparing the array representation of the frame where the game will be captured\n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3].astype(np.uint8)\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (100,83))\n",
    "        channel = np.reshape(resized, (1,83,100))\n",
    "        return channel\n",
    "\n",
    "    #For the agent to understand when the Game is considered to be over\n",
    "    def get_done(self):\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))\n",
    "        done_strings = ['GAME', 'GAHE']\n",
    "        done=False\n",
    "        # if np.sum(done_cap) < 44300000:\n",
    "        #     done = True\n",
    "        #done = False\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done = True\n",
    "        return done, done_cap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531828ad-b178-4d79-89ed-32bf43567e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7d12d-f2f0-4e25-83df-7d13137bc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DinoGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20789fba-c6e9-456d-a72e-f45cc72ef4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61cd66-a180-4916-9ad0-5e2bc2eb556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(obs[0], cv2.COLOR_GRAY2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766cae1-6c50-4ed7-b83a-c1915a1e8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "done, done_cap = env.get_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845ed56-a7a1-4e8a-8252-16a87918e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(done_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244628c-7ff4-499f-a079-94bfaa888ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.image_to_string(done_cap)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cced49-7cba-4ab6-889b-7b8e304c4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False  \n",
    "    total_reward   = 0\n",
    "    while not done: \n",
    "        obs, reward,  done, info =  env.step(env.action_space.sample())\n",
    "        total_reward  += reward\n",
    "    print('Total Reward in the episode {} is {}'.format(episode, total_reward))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee0d65-0f42-4122-b430-3aca056399c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d7ffa6-d8a5-40df-a960-161d74e49278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a callback for checking the environment before model training\n",
    "\n",
    "import os \n",
    "from stable_baselines3.common.callbacks import BaseCallback  \n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c17e0e-e09e-4235-a21e-950f02f193f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4cfd2-3227-4243-a85b-b0161f9405aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every epoch of training and logging the results of each step\n",
    "class TrainAndLogCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLogCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c21d3-6d64-486e-b103-9e8c25ea85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories for storing training checkpoints and logs\n",
    "check_dir = './train/'\n",
    "log_dir = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab84ef-d6fc-4a7c-b7e9-5bc8efe8d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLogCallback(check_freq=1000, save_path=check_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95465ec-f3a5-414e-81f6-7f239d098790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Developing the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7816b-d7b6-4afb-a42b-7f0c4a887b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing DQN algorithm for model training\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f31f5-3472-4476-b1dc-14fa5568b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DinoGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997edb4f-8f8d-45e3-8a27-1e1ae82251dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('CnnPolicy', env, tensorboard_log=log_dir, verbose=1, buffer_size=1000000, learning_starts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae5e2d-a3d7-4444-bb3d-6e08e3b848b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=85000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a7218-bfdd-4e0f-94e9-956f490035b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('train_first/best_mode l_50000') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d6533-9651-4edf-8fa2-b4f6302b9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e3e88-703b-4f53-9111-0aa53cb9f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))   #Allows the agent to play the game\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
